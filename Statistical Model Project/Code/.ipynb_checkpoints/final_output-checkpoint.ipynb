{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta  \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import xlsxwriter\n",
    "import more_itertools as mit\n",
    "import math\n",
    "import time\n",
    "\n",
    "#  ---------------------- Added by NM on Aug, 31, 2020 ----------\n",
    "\n",
    "#  Adding DB call using databas.ini and conf.py\n",
    "\n",
    "import sys\n",
    "# from config import config\n",
    "from datetime import date, datetime\n",
    "# import psycopg2\n",
    "# import psycopg2.extras\n",
    "import time\n",
    "from functools import singledispatch\n",
    "from os import listdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making connection with sqlite db\n",
    "import sqlite3\n",
    "def config():\n",
    "    try:\n",
    "        conn = sqlite3.connect('public.db')\n",
    "        return conn\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Connect into DB and Select data from DBA --hs_pipeline table\n",
    "def db_conn():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn_parameters = config()\n",
    "        conn = psycopg2.connect(**conn_parameters)\n",
    "        print(conn)\n",
    "        cur = conn.cursor()\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        print(\"Not able to connect into the Database {}\\n\".format(error)) \n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            print(\" DB is connected!\\n\")\n",
    "\n",
    "    get_pid_id=\"0001_0001\"\n",
    "    hs_table_pipleline = \"hs_16_clean_pipeline_SC_\"\n",
    "    hs_table = ''.join(hs_table_pipleline+get_pid_id) #hs_16_clean_pipeline_SC_0001_0001\n",
    "\n",
    "    # Let us Select the data from the DB pipeline  and see what would happen\n",
    "\n",
    "    hs_fetch_data_statement = \"select * from public.\\\"%s\\\" ORDER BY timestamp DESC limit 1;\" %(hs_table)\n",
    "    print(\"We are selecting data from Humidity pipleine table:\\n\", hs_fetch_data_statement)\n",
    "    dh_stat = pd.read_sql_query(hs_fetch_data_statement, con=conn)\n",
    "\n",
    "    df_stat_clean = list(dh_stat.values)\n",
    "    print(df_stat_clean)\n",
    "    \n",
    "    data_dh = pd.DataFrame(df_stat_clean[:1])\n",
    "\n",
    "    dh_stat_numpy = np.array(data_dh.iloc[:,3:243])\n",
    "    dh_stat_timestamp = np.array(data_dh.iloc[:,2:3], dtype=str)\n",
    "      \n",
    "    #print(dh_stat_numpy)\n",
    "    \n",
    "    return dh_stat_numpy, dh_stat_timestamp\n",
    "\n",
    "# Call Humidity DB connec & Select data from DBA Humidity pipeline table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get start time of each interval\n",
    "def get_intervals(time_overlap, data):\n",
    "    \n",
    "    start_time = []\n",
    "    \n",
    "    file_start_time = data[\"timestamp\"][0]\n",
    "    \n",
    "    file_end_time = data[\"timestamp\"][len(data.values)-1]\n",
    "    \n",
    "    start_time.append(file_start_time)\n",
    "    \n",
    "    while start_time[-1] + timedelta(seconds=time_overlap*60) <= file_end_time:\n",
    "        \n",
    "        start_time.append(start_time[-1] + timedelta(seconds=time_overlap*60))\n",
    "        \n",
    "    return start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(start_time, end_time, data, sensor_columns):\n",
    "    \n",
    "    #greater than the start date and smaller than the end date\n",
    "    mask = (data['timestamp'] > start_time) & (data['timestamp'] <= end_time)\n",
    "    \n",
    "    return data.loc[mask][sensor_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get average change of top 6 sensors\n",
    "def get_avg_change_top_6(data):\n",
    "    \n",
    "    change_in_sensors = {}\n",
    "    \n",
    "    # calculate average change in each sensor of center zone \n",
    "    for column in data.keys():\n",
    "    \n",
    "        change = [(abs(b - a)/(a+0.1)) for a, b in zip(data[column][1:].astype('float64'),data[column][2:].astype('float64'))]\n",
    "    \n",
    "        change_in_sensors[column] = np.mean(change)\n",
    "        \n",
    "    # get top 6 sensors\n",
    "    top_6 = dict(sorted(change_in_sensors.items(), key=operator.itemgetter(1),reverse=True)[:6])\n",
    "    \n",
    "    # get average change of top 6 sensors\n",
    "    avg_change = 0\n",
    "\n",
    "    for key in top_6:\n",
    "    \n",
    "        avg_change += top_6[key]\n",
    "    \n",
    "    avg_change = avg_change/float(6)\n",
    "    \n",
    "    return avg_change\n",
    "\n",
    "  # Top 6 sensors means the six sensors that will have maximum change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get percent change from last interval\n",
    "def get_percent_change(current_record, previous_record):\n",
    "    \n",
    "    percent_change_in_sensors = {}\n",
    "    \n",
    "    # calculate percent change in each 16 perimeter reference sensor \n",
    "    for column in current_record.keys():\n",
    "    \n",
    "        percent_change = [(abs(a - b)/(b+0.1))*100.0 for a, b in zip(current_record[column][1:].astype('float64'), previous_record[column][1:].astype('float64'))]\n",
    "    \n",
    "        percent_change_in_sensors[column] = np.mean(percent_change)\n",
    "        \n",
    "    # get top 6 sensors\n",
    "    top_6 = dict(sorted(percent_change_in_sensors.items(), key=operator.itemgetter(1),reverse=True)[:6])\n",
    "    \n",
    "    # get average change of top 6 sensors\n",
    "    percent_avg_change = 0\n",
    "\n",
    "    for key in top_6:\n",
    "    \n",
    "        percent_avg_change += top_6[key]\n",
    "    \n",
    "    percent_avg_change = percent_avg_change/float(6)\n",
    "    \n",
    "    return percent_avg_change   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function for extracting required timestamps and labels\n",
    "def extract_data(timeslots, actual_labels, ts):\n",
    "    \n",
    "    timestamps = []\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    convert_timestamp = ''\n",
    "    \n",
    "    interval = 0\n",
    "    \n",
    "    for k in timeslots.keys()[1:]:\n",
    "        \n",
    "        # if date or time is NULL\n",
    "        if type(timeslots[k][ts]) == float and math.isnan(float(timeslots[k][ts])):\n",
    "            \n",
    "            # if date is NULL, no need to save time\n",
    "            if \"Date\" in k:\n",
    "            \n",
    "                flag = -1\n",
    "                \n",
    "            # if date is available but time is NULL just empty the converted timestamp\n",
    "            else:\n",
    "                \n",
    "                convert_timestamp = ''\n",
    "        else:\n",
    "                    \n",
    "            # get time\n",
    "            if \"Assessment\" in k:\n",
    "        \n",
    "                time = timeslots[k][ts]\n",
    "            \n",
    "                convert_timestamp += str(time)\n",
    "                \n",
    "                if flag >= 0:\n",
    "            \n",
    "                    # Convert string to datetime object\n",
    "                    converted_timestamp = datetime.strptime(convert_timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "                    # Save all converted timestamps and labels\n",
    "            \n",
    "                    if actual_labels[\"T\"+str(interval+1)][ts] == \"dry\":\n",
    "                    \n",
    "                        labels.append(0)\n",
    "            \n",
    "                        timestamps.append(converted_timestamp)\n",
    "                \n",
    "                    elif actual_labels[\"T\"+str(interval+1)][ts] == \"wet\":\n",
    "                        \n",
    "                        labels.append(1)\n",
    "                        \n",
    "                        timestamps.append(converted_timestamp)\n",
    "                        \n",
    "            \n",
    "                    convert_timestamp = ''\n",
    "                \n",
    "                    flag = 0\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    flag = 0\n",
    "            \n",
    "                interval += 1\n",
    "            \n",
    "            # get date\n",
    "            else:\n",
    "            \n",
    "                # if year is first in the date\n",
    "                if len(str(timeslots[k][ts]).split(\"-\")[0]) > 2:\n",
    "                \n",
    "                    convert_timestamp = str(timeslots[k][ts]).split(\"-\")[0] + \"-\" + str(timeslots[k][ts]).split(\"-\")[2].split(\" \")[0] + \"-\" + str(timeslots[k][ts]).split(\"-\")[1] + \" \" \n",
    "                \n",
    "                # if year is not first in the date\n",
    "                else:\n",
    "            \n",
    "                    date = str(timeslots[k][ts])\n",
    "                \n",
    "                    convert_timestamp = date.split(\"-\")[2] + \"-\" + date.split(\"-\")[1] + \"-\" + date.split(\"-\")[0] + \" \"\n",
    "             \n",
    "                \n",
    "    return timestamps, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(time_window, time_overlap, database_name , sensor_columns, threshold, percent_change):\n",
    "\n",
    "    #table_names  = get table names\n",
    "    \n",
    "    #e.g. table_names = [hs_16_clean_pipeline_SC_0001_0001]\n",
    "    \n",
    "#     executing loop for each table(for each patients)\n",
    "    for table in table_names:\n",
    "        \n",
    "       \n",
    "        conn = config()\n",
    "        \n",
    "        #data  = \"select * query from hs_16_clean_pipeline_SC_0001_0001\"\n",
    "        data = pd.read_sql(\"select * from \"+table,conn)\n",
    "        # get patient id\n",
    "        pid = table.split( \"ref_val_16_SC_\" )[-1]\n",
    "        \n",
    "        # convert timestamp from string to datetime\n",
    "        data[\"timestamp\"] = pd.to_datetime(data['timestamp'], format='%Y-%m-%d %H:%M:%S') \n",
    "        \n",
    "        interfaceid = data.iloc[0][\"interfaceid\"] \n",
    "        gatewayid = data.iloc[0][\"gatewayid\"] \n",
    "        \n",
    "        # get intervals in file\n",
    "        intervals_start = get_intervals(time_overlap, data)\n",
    "        \n",
    "        intervals = 0 # number of intervals processed\n",
    "        saved_intervals = [] # saved processed intervals\n",
    "        patient = {} # dictionary for creating a dataframe for each patient\n",
    "        \n",
    "        patient[\"Patient ID\"] = pid\n",
    "#         list for dumbing record in db table \"cur_app_clinical_events\"\n",
    "        dump_db = [  ]\n",
    "        \n",
    "        \n",
    "        for start_time in intervals_start:\n",
    "            \n",
    "            # get records of center zone sensors for each interval\n",
    "            records = get_records(start_time, start_time + timedelta(seconds=time_window*60), data, sensor_columns)\n",
    "            \n",
    "            patient[\"Interval Time Start\"] = start_time\n",
    "            \n",
    "            patient[\"Interval Time End\"] = start_time + timedelta(seconds=time_window*60)\n",
    "            \n",
    "            # get average change of top 6 sensors\n",
    "            avg_change = get_avg_change_top_6(records)\n",
    "            \n",
    "            patient[\"Avg. change of top6\"] = avg_change\n",
    "            \n",
    "            # get percent change from last interval\n",
    "            if intervals == 0:\n",
    "                \n",
    "                percent_change_last_interval = 0\n",
    "                \n",
    "                patient[\"Percent Change from last interval\"] = percent_change_last_interval\n",
    "                \n",
    "                patient[\"flag\"] = 0\n",
    "                \n",
    "                df = pd.DataFrame([patient], columns = ['Patient ID', 'Interval Time Start', 'Interval Time End', 'Avg. change of top6', 'Percent Change from last interval', 'flag'])\n",
    "                \n",
    "                print(\"columns\",patient.keys())\n",
    "                \n",
    "                dump_db.append( [ interfaceid , gatewayid ,  start_time  , patient[\"Percent Change from last interval\"]  , patient[\"flag\"] ] )\n",
    "                \n",
    "                if os.path.isdir('./data/tw_'+str(time_window) + \"_to_\" + str(time_overlap) + \"_th_\" + str(threshold)+ \"_pc_\" + str(percent_change)):\n",
    "\n",
    "                    df.to_csv('./data/tw_'+str(time_window) + \"_to_\" + str(time_overlap)+ \"_th_\" + str(threshold)+ \"_pc_\" + str(percent_change)+\"/\"+pid+\"_Results\"+'.csv', mode='w', header=True)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    os.makedirs('./data/tw_'+str(time_window) + \"_to_\" + str(time_overlap)+ \"_th_\" + str(threshold)+ \"_pc_\" + str(percent_change))\n",
    "                    \n",
    "                    df.to_csv('./data/tw_'+str(time_window) + \"_to_\" + str(time_overlap)+ \"_th_\" + str(threshold)+ \"_pc_\" + str(percent_change) + \"/\"+pid+\"_Results\"+'.csv', mode='w', header=True)\n",
    "    \n",
    "            else:\n",
    "                \n",
    "                percent_change_last_interval = get_percent_change(records, saved_intervals[-1])\n",
    "                \n",
    "                patient[\"Percent Change from last interval\"] = percent_change_last_interval\n",
    "                \n",
    "                # flag row if percent change from last interval is greater than 70\n",
    "                if percent_change_last_interval > percent_change:\n",
    "                    \n",
    "                    patient[\"flag\"] = 1\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    patient[\"flag\"] = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "                dump_db.append( [ interfaceid , gatewayid ,  start_time  , patient[\"Percent Change from last interval\"]  , patient[\"flag\"] ] )\n",
    "                \n",
    "                \n",
    "                df = pd.DataFrame([patient], columns = ['Patient ID', 'Interval Time Start', 'Interval Time End', 'Avg. change of top6', 'Percent Change from last interval', 'flag'])\n",
    "                    \n",
    "                df.to_csv('./data/tw_'+str(time_window) + \"_to_\" + str(time_overlap)+ \"_th_\" + str(threshold)+ \"_pc_\" + str(percent_change)+\"/\"+pid+\"_Results\"+'.csv', mode='a', header=False)\n",
    "                    \n",
    "            \n",
    "            \n",
    "                \n",
    "            intervals += 1\n",
    "            \n",
    "            saved_intervals.append(records)\n",
    "            \n",
    "#         print( dump_db )\n",
    "#       dumping result records in db table \"cur_app_clinical_events\"\n",
    "        pd.DataFrame(dump_db,columns=[\"interfaceid\",\"gatewayid\",\"timestamp\",\"percent_change\",\"event_type\"]).to_sql(\"cur_app_clinical_events\",config(),if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for validation and table creation\n",
    "import sqlparse\n",
    "def create_table(path=\"../Data Model & Schema/Database-A.sql\"):\n",
    "    cur = None\n",
    "    table_str = \"\"\n",
    "    with open(path) as schema_file:\n",
    "        create_table = schema_file.readlines()\n",
    "        validation_string = \"\".join(create_table).replace('\"','')\n",
    "        try:\n",
    "            validation_string = sqlparse.parse(validation_string)\n",
    "        except Exception as e:\n",
    "            print(\"Bad Statement : \",e )\n",
    "        try:\n",
    "            conn = config()\n",
    "        except:\n",
    "            return cur\n",
    "        cur = conn.cursor()\n",
    "        for table in validation_string:\n",
    "            try:\n",
    "                print(type(table))\n",
    "                cur.execute(str(table))\n",
    "            except Exception as e:\n",
    "                print(\"error in creating table\")\n",
    "                print(e)\n",
    "    return cur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for dumping json data into tables\n",
    "def dump_data():\n",
    "    center_sensor = pd.read_json(\"../JSON/_hs_16_clean_SC_0001_0001__202008281500.json\")\n",
    "    ref_sensor = pd.read_json(\"../JSON/_ref_val_16_SC_0001_0001__202008281501.json\")\n",
    "    ref_sensor = pd.DataFrame(ref_sensor.ref_val_16_SC_0001_0001.to_list())\n",
    "    center_sensor = pd.DataFrame(center_sensor.hs_16_clean_pipeline_SC_0001_0001.to_list())\n",
    "    ref_sensor = ref_sensor.drop([\"RefT_1\",\"RefT_2\",\"RefT_3\",\"RefT_4\",\"RefT_5\",\"RefT_6\",\"RefT_7\",\"RefT_8\",\n",
    "                                  \"RefT_9\",\"RefT_10\",\"RefT_11\",\"RefT_12\",\"RefT_13\",\"RefT_14\",\"RefT_15\",\"RefT_16\"]\n",
    "                                  ,axis=1)\n",
    "    conn = config()\n",
    "    ref_sensor.to_sql(\"ref_val_16_SC_0001_0001\",conn,if_exists='replace')\n",
    "    center_sensor.to_sql(\"hs_16_clean_pipeline_SC_0001_0001\",conn,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sqlparse.sql.Statement'>\n",
      "<class 'sqlparse.sql.Statement'>\n",
      "<class 'sqlparse.sql.Statement'>\n",
      "<class 'sqlparse.sql.Statement'>\n",
      "<class 'sqlparse.sql.Statement'>\n"
     ]
    }
   ],
   "source": [
    "# creating tables in database for Gateway Input\n",
    "create_table(\"../Data Model & Schema/Database-A.sql\")\n",
    "create_table(\"../Data Model & Schema/Database-C.sql\")\n",
    "dump_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'CenterZone sensors'\n",
    "sensor_columns = [ \"H_93\",\"H_103\",\"H_113\",\"H_123\",\"H_133\",\"H_143\",\n",
    "                   \"H_94\",\"H_104\",\"H_114\",\"H_124\",\"H_134\",\"H_144\",\n",
    "                   \"H_95\",\"H_105\",\"H_115\",\"H_125\",\"H_135\",\"H_145\",\n",
    "                   \"H_96\",\"H_106\",\"H_116\",\"H_126\",\"H_136\",\"H_146\",\n",
    "                   \"H_97\",\"H_107\",\"H_117\",\"H_127\",\"H_137\",\"H_147\",\n",
    "                   \"H_98\",\"H_108\",\"H_118\",\"H_128\",\"H_138\",\"H_148\" ]\n",
    "# reference_sensors\n",
    "ref_columns = ['RefH_1', 'RefH_2','RefH_3', 'RefH_4', 'RefH_5', 'RefH_6', 'RefH_7', 'RefH_8', 'RefH_9',\n",
    "               'RefH_10', 'RefH_11', 'RefH_12', 'RefH_13', 'RefH_14', 'RefH_15','RefH_16']\n",
    "\n",
    "# # Parameter Tuning\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "time_window = 15      # difference of End and Start timestamp in a event\n",
    "\n",
    "time_overlap = 1     # difference among the start timestamp of two consecutive rows\n",
    "\n",
    "threshold = 10        # criteria of picking up the consecutive number of rows\n",
    "\n",
    "percent_change = 1   # percentage change for an interval\n",
    "\n",
    "performance = {} # for storing accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns dict_keys(['Patient ID', 'Interval Time Start', 'Interval Time End', 'Avg. change of top6', 'Percent Change from last interval', 'flag'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umair/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/umair/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "table_names = [\"ref_val_16_SC_0001_0001\"]\n",
    "# perform statistical analysis on all the files in the given tables\n",
    "stats(time_window, time_overlap, table_names, ref_columns, threshold, percent_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its done\n"
     ]
    }
   ],
   "source": [
    "print(\"its done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
